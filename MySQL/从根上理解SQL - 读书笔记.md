

# Chap9 表空间
InnoDB是以页为单位管理存储空间 聚簇索引以及二级索引都是以B+树的形式存在表空间里面 而B+树的节点就是数据页<br>
页面通用结构：首尾分别为File Header、File Trailer
- File Header记录一些页面的通用信息
- File Trailer校验页是否完整

## 独立表空间结构
区(extent): InnoDB中为了更好的管理页 设计连续的64个页为一个区 一页为16KB 因此一个区默认占1MB的空间 同时256个区被划分为一组 每个表空间都可以看做是若干个区组成的 同时每个组最开始的几个页面类型是固定的为XDES IBUF_BITMAP 第一个组中还有FSP_HDR这个类型的页面 用于记录整个表空间的一些整体属性 以及 本组所有的区的属性

段(segment): B+树的叶子节点和非叶子节点分开存储 存放叶子节点的区的集合算一个段

设计 区 和 段 的目的：
- 区: 由于表空间主要存储的就是聚簇索引以及二级索引这些数据 我们往表中插入数据 聚簇索引以及二级索引都需要插入数据 若表中数据量很大 需要存很多页 则我们在插入数据时 需要找到插入的位置 又因为页并不是连续的 因此 会造成插入数据性能低下(在插入数据时 要根据插入的值利用二级索引查找主键值 找到主键值后 从节点之间的链表遍历到查找的位置 由于表在物理空间上并不是连续存储的 所以这个遍历过程可以看作随机I/O 效率低下) 因此我们设计区为连续的页面 把数据存在连续的页面中提高性能 虽然当数据量少的时候 由于区的大小固定可能会造成空间浪费 但是从性能角度上看 功大于过
- 段: 之前提到的遍历 其实是对叶子节点的遍历 而不用管非叶子节点 因此将两部分数据分成两个区存 这就是段了 所以一颗B+树会生成两个段 一个是叶子节点段 另一个是非叶子节点段 但是当表中数据较少时 若按上面的分配方式 至少需要为一张表分配2M的空间 过于浪费 因此又设计了碎片区 碎片区的页并不单一的属于某一个段 而是分属于不同的段 碎片区直属于表空间 并不属于任何一个段
  - 根据如上规则插入数据：
  - 在刚开始向表中插入数据时 段是从某个碎片区以单个页面为单位来分配存储空间
  - 当某个段已经占用32个碎片区的页面 即一半后 就会以完整的的区为单位来分配空间
- 所以段的定义为某些零散的页面以及一些完整的区的集合

表空间是由若干个区组成的 可以划分以下四种类型
- 空闲的区(FREE)：没有用到任何区中的页面
- 有剩余空间的碎片区(FREE_FRAG)：仍有空闲页面的碎片区 
- 没有剩余空间的碎片区(FULL_FRAG)：碎片区中的所有页面都被使用
- 附属于某个段的区(FSEG)：用于记录某一个索引的叶子节点或非叶子节点的段中的区

前三者是独立的，直属于表空间; FSEG是附属于某个段的

XDES Entry(Extent Descriptor Entry): 用于记录区的属性，共40字节，分为以下4部分
- Segment ID(8字节)：若该区属于某个段 则记录段的ID 否则无意义
- List Node(12字节): 用于记录指向前一个XDES和后一个XDES的指针 即利用这个字段将区和区之间像链表一样连起来 定位一个XDES需要两个信息：next node page number 以及 next node offest 即找到下一个节点所属的页面以及页内偏移
  - <details> <summary> XDES Entry链表的作用 </summary> XDES Entry链表主要作用为快速定位到XDES Entry。<br> 1. 对于直属于表空间的三种区，段中数据较少时，插入数据，首先寻找表空间中的FREE_FRAG的区 若存在 则插入数据 否则申请一个FREE的区 然后把该区变为FREE_FRAG。之后不同的段会从该区申请页面 直到没有空闲的也买你 把状态改为FULL_FRAG。当表空间数据量很大时 查找是否存在FREE_FRAG的区若是通过遍历表空间所有区的状态 会导致效率十分低下 因此考虑将直属于表空间的三种状态的区连接成三个链表: FREE/FREE_FRAG/FULL_FRAG; 这样想要查找FREE_FRAG的区的时候 直接访问FREE_FRAG链表的头节点即可 当该节点中的页面用完的时候 就修改state值 然后移动到FULL_FRAG链表中即可 这样就大大提升了效率。<br> 2. 对于附属于某个段的区，当段中数据已经沾满32个零散的页面后 申请一个完整的区来插入数据。此时我们需要找到属于该段的区存储数据 如何找到属于该段的区？ 利用链表 但这里构建链表是根据XDES Entry中的Segment ID属性， 因为不同的段中的数据不会存在同一个区中 同时一个段中的区也有三种状态: FREE/ NOT FULL/ FULL 因此每个段都需要维护三个链表 而一个索引对应两个段 因此一个索引需要维护6个链表 同时不要忘记直属于表空间中的3个链表 <br> 3. 链表基节点。链表基节点用于定位链表 存有以下信息 <br> - List length(4字节) 表明该链表中有多少节点 <br> - First node page number 和 First node offest表明该链表的头节点在表空间中的位置 <br> - Last node page number 和 Last node offest表明该链表的尾节点在空间的位置 <br> 最后链表基节点会存在表空间中的固定位置便于访问
- State(4字节): 记录区的状态 即之前那4个状态(emmmm竟然只用记录4总状态 为什么需要4字节？？？)
- Paeg State Bitmap(16字节): 记录区中64个页面的信息 16字节一共128bit 所以一个页面的信息占2bit  一个比特位用于记录页面是否空闲 另一个还没有用

段的结构：<br>
之前给过段的定义：一些零散的页面以及一些完整的区的集合。下面给出段的结构用于记录段中的这些信息， 这个结构叫做INODE Entry<br>
- Sgemment ID: 段的ID
- NOT_FULL_N_USED: 用于记录NOT_FULL链表中已经使用了多少页面 这样就不用从链表的第一个页面开始遍历寻找空闲页面
- FREE/NOT_FULL/ FULL三个链表基节点
- magic number: 标记这个INODE Entry是否初始化
- Fragment Array Entry: 用于记录段中零散的页面的页号 一共记录32个页号 每个4字节 因为当零散的页面超过32个的时候 就会申请一个完整的区用于记录数据

表空间整体结构：表空间由组构成 每个组有256个区 每个区有64个页面 <br>
其中第一个组的第一个区的第一个页面比较特殊 会记录表空间的整体属性 这个页面是FSP_HDR(File Space Header) 和其他组的第一个页面相比 多了一部分就叫做File Space Header的信息 当然其他组的第一页中也有这部分空间 不过为空<br>
- File Space Header(第一组第一页特有)：
    - 表空间的三个链表基节点
    - FRAG_N_UESD 表示链表Free_FRAG 中已经使用的页面数量 方便之后在链表中查找空闲的页面
    - FREE_LIMIT 表示初始化时加入FREE链表的页面数量 一开始创建表空间的时候 不会讲所有的空闲区都加入链表 而是至添加一部分 等到后续不够了 再添加
    - NEXT UNUSED Segment ID 每个索引都对应两个段 每个段都有一个叫做Segment ID的参数 通过next unused id来赋值 否则的话需要遍历空间中所有的ID找到没有被用过的ID赋值 降低效率
    - List Base Node of SEG_INODES_FULL/FREE List 用于存放INODE类型页面链表的基节点 INODE类型的页面适用于存储 INODE Entry结构 这个结构是段的信息结构 用于存放段的信息 包括三条链表的基节点以及零碎页面的信息
- XDES Entry
  - 这个部分主要存储当前组中256个区的信息 由于一个页面的大小有限 因此限制一个组为256个区 同时只有每个表空间的第一个组存在 FSP_HDR这个类型的页面存放XDES Entry部分 其余组的第一个页面都直接就是XDES类型 用于存储XDES Entry信息 结构和FSP_HDR类似 不过少了File Space Header部分的信息


## 系统表空间结构


# Chap10 单表访问方法
这章主要将对一个表进行查询操作时 Mysql会如何执行查询优化

访问方法主要有几下几种

1. const

主键列(聚簇索引) 或者 唯一二级索引(即每个索引值是唯一的) 和一个常数进行等值比较时使用const方法

2. ref

普通的二级索引 与 常数 进行等值比较

3. ref_or_null

二级索引不仅想找出某个常数值的记录 还有 null的记录

4. range

索引列匹配某些值 或者 某个范围的值 时使用range

5. index

遍历二级索引的查询 即不需要返回所有列的数据 查找的数据被二级索引列覆盖即可

6. all

全表遍历 即对聚簇索引遍历查找

Remark:
- 一般情况下只能用一个二级索引进行查询 当你要查两个不同的列时 会先利用一个二级索引进行查询 然后回表 在聚簇索引中利用主键值进一步过滤第二个条件
- 在range查询中 只要索引和常数使用操作符= <= >= .... in like等连起来 就可以产生一个区间 不同的条件需要使用AND 或者 OR连接起来 
  - 当所有的搜索条件都可以用一个二级索引进行查询 比如 where key2 > 100 AND key2 > 200 此时查询会取交集  若是OR 则回去并集
  - 有的搜索条件没有索引时 比如 where key2 > 100 AND common filed = 'abc' 其中只有Key2建立了索引 则查询优化会将其替换为TRUE 即先不考虑 这个条件 但是若这里是OR 就会变成了key2 > 100 OR TRUE = TRUE 就变成了遍历二级索引了 这样回表的操作就会很低效 即一个使用到索引的搜索条件和一个没有使用索引的搜索条件用OR连接起来后是无法使用索引的
- 索引合并
  - 之前说过一般情况下只会使用一个二级索引 但有些特殊情况是会使用多个的
  - Intersection合并 即将两个二级索引的搜索结果的主键值先求交集 再回表查询 只有下列情况可能会使用索引合并(注意 这里是可能 就是说就算满足这些条件也可能不使用索引合并进行查询优化)
    - 二级索引列都是等值匹配的情况
    - 主键列可以是范围匹配
    - 这里可以用的原因主要在于当索引列都是等值匹配的时候 回表的主键值其实是递增排序的 因此求两个的交集是O(n)的复杂度 很快
  - Union合并 上面是求交集 这里就是求并集了
    - 二级索引列都是等值匹配
    - 主键可以是范围匹配
    - 使用Intersection索引合并的搜索条件 就是说有两部分 每部分都是Intersection的结果 然后这两部分用OR连接
  - Sort-Union合并 整体也是求并集 不过这里不是等值匹配 而是范围 是将范围搜索得到的结果进行排序后再按照Union合并
    - 这总方式比单纯的Union合并多了一步排序的的工作
    - 注意到Intersection合并是没有Sort的  因为Union合并的适用场景是从某个二级索引中获取 的记录比较少 这样就算排序也不会有太大的消耗 而Intersection合并 是值单个的索引获取的记录太多 通过求交的方式减少一部分 如果还要排序 消耗就太大了





# Chap11 连接
连接就是把两个表连在一起进行查询 有时不同的信息存在不同的表中 使用连接可以同时对两个表中的结果进行过滤查询


## 内连接
INNER JOIN (= JOIN = CROSS JOIN)
内连接即将两个表的所有记录以组合的方式返回所有的可能 也叫做笛卡尔直积

这种连接方式若不加过滤条件 最终得到的结果可能非常大 比如3个100条记录的表进行连接就能得到100 x 100 x 100 = 1000000条记录 因此需要加上where条件对表中的记录进行过滤

## 外连接
LEFT JOIN 
RIGHT JOIN
与内连接相比 外连接会区分两个表 比如我们需要对两个表进行连接 并设置了过滤条件 在内连接中 若表1的记录在表2中找不到匹配的记录 则不会加入中返回的结果中 但是外连接则不同 若确定表1为驱动表 表2为被驱动表 若在表2中找不到匹配的记录 表1中的记录仍然会加入返回的结果 不过表2中的那些列为nuLL值

上面提到的匹配的意思：表1中存储学生信息 比如学号 姓名 选课等信息 表2中存储课程 学号 成绩等信息 

此时若想查询每个学生的乘积 由于表1中没有乘积 表2中没有姓名 因此需要对两个表进行连接查询得到完整的信息  此时若是表1中的学生缺考 则在表2中不会有成绩记录 也就出现了表1中的记录在表2中找不到匹配的记录这一情况 若是内连接 则缺考的学生的信息不会加入返回的结果中 而若是外连接 并将表1设为驱动表 则会将所有表1中的记录加入结果 即使在表2中找不到匹配的信息 

上面的匹配过程用的过滤的命令都是ON 这是专门针对外连接的命令 若是使用where则过滤掉的表1中的记录 一样不会出现在返回的结果中

同时对于内连接来说 ON = WHERE


## 连接查询的原理
本质：先对驱动表进行查询 返回满足条件的记录 对每一条驱动表中满足条件的记录 都会在被驱动表中进行查询 所以 **驱动表只查询一次 被驱动表可能被访问多次 访问次数取决于对驱动表执行单表查询后的结果集中的记录条数**

- 优化

1. 由于连接本质上还是进行查询 所以可以通过设置索引加快查询速度
2. 减少访问被驱动表的次数 首先在之前的查询中 我们是对每一条查询驱表返回的结果 都在被驱动表中进行一次查询 每一次都会重新从内存中加载被驱动比中的记录与驱动表返回的结果进行匹配 为了减少从内存中读取被驱动表记录的次数 提出了一个JOIN BUFFER的概念 先把若干条驱动表返回的结果加载到buffer中 然后每条被驱动表中的记录 一次性与buffer中所有的驱动表的记录进行匹配 这样就减少了访问被驱动表的次数



# Chap12 基于成本优化(成本计算)


# Chap13 表和索引的统计数据



# Chap19 事务
事务对应 一个或多个 数据库操作 就是一系列针修改数据库的语句 叫做一个事务(transaction) 从英文的角度理解 就是一笔交易 进行这笔交易会对数据库中的信息产生影响

事务的四种性质：ACID
- 原子性atomicity: 即事务中的修改要么都完成 要么都不完成 
  - 对应现实中的交易就是说 一手交钱一手交货 不能只交钱不交货 也不能只交货不交钱
- 一致性consistency: 即事务中的记录保持一致 比如说交易前后交易双方的总额不变 比如说每个人的余额都大于0
- 隔离性isolation: 即两个不同的事务之间是分开进行的 不会相互影响
- 持久性durability: 即一个事务一旦完成 这种修改是不会改变的 

事务的五种状态：
- 活动的：当事务正在运行中
- 部分提交的：当事务的操作都完成了 但是由于是在内存中进行 因此还没有更新到磁盘中 此时称为部分提交的
- 失败的：当事务处于活动的 或者 部分提交的时候遇到了某些错误导致事务无法继续运行 称为失败的
- 中止的：当事务处于失败的时候 由于原子性 我们需要对事务进行回滚 回滚到没有开始前的状态 回滚后称为中止的
- 提交的：当事务全部完成 并将内存中的数据更新到磁盘后 我们称为提交的

只有当事务出于中止的或提交的 一个事务的生命周期才算结束

事务语法：
- 创建事务
  - begin [work]  // work为事务的名称 自定义 可以省略
  - start transaction // 这个和上面的效果一样 不同之处在于 这个可以自己加参数
    - read only
    - read write
    - with consistent snapshot
- 提交事务
  - commit [work]
- 手动中止事务
  - rollback [work]
    - rollback是直接回滚所有的事务 若只需要回滚部分语句 可以设置savepoint相当于存档点 
    - 存档： savepoint point_name
    - rollback [work] to [savepoint] point_name
- 自动提交
  - 这块比较多 不写了



# Chap20 redo日志 - 内存
redo日志的由来：<br>
之前说过事务具有持久性 为了保持事务的持久性 需要将对表做的修改同步到磁盘中去。有些时候可能只更改了表中的一条数据，但是表中却要更改很多数据 比如索引树增加一条记录 表空间头信息等改变 索引树的目录等 总之需要有很多信息需要改变 由于以下两个约束
- 由于表中的数据存储的基本单位是页 所以我们可以选择直接将表中修改过的页一整页都更新到磁盘中去 但是由于修改的数据毕竟在一整页中占少数 因此这样做过于浪费时间(磁盘和内存之间的交互很浪费时间 刷新了这么多没有变化的数据 浪费的时间就很多了)
- 由于表中的数据可以由表空间号+页号+偏移量定位 所以可以选择定位到刷新的部分 只刷新更改的那一部分数据 但是由于更新数据带来的修改可能在不同的页面 比较分散 因此这样刷新 就相当于随机访问磁盘的某一个位置 这样就更慢了 所以也不能接受

由于上面的约束，因此想出了一种新的刷新方式，每次更新数据后将表中更新了那些位置记录下来 形成redo日志 然后把这些redo日志刷新到磁盘上去 由磁盘上的函数以这些redo日志为输入参数更新磁盘上的数据 这样由于redo日志是连续的内存空间 且都是有用的内容 因此就规避了上述两个问题 因此这里需要引入redo日志

redo日志的介绍分为两部分：
- 内存中 首先我们在内存中更新数据 然后生成redo日志 
- 磁盘中 将内存中的redo日志 刷新到磁盘中的redo日志 


## redo日志的结构

- type: redo日志的类型
  - 总体上可以分为两类日志
  - 一类物理日志，记录一下在某个页面的某个偏移量处修改了几个字节的值，具体被修改的内容是啥就好了，即一条日志对应一个地方简单的修改数值 此时
  redo日志的类型一般有MLOG_1BYTE, MLOG_2BYTE, MLOG_4BYTE, MLOG_8BYTE, MLOG_WRITE_STRING 分别表示在对应位置写入1个字节、2个字节、4个字节、8个字节、字符串类型的修改数据<br>
  而且此时data部分为offset + data / offset + len + data(string类型)
  - 另一类是逻辑日志，这类日志并不直接记录修改的数据 而是像之前说的 记录参数 然后由磁盘中的函数以这些记录为参数 修改磁盘中的数据<br>
  这些类型由MLOG_REC_INSERT, MLOG_COMP_REC_INSERT....在MYSQL5.7.21中包括物理以及逻辑日志一共有53中不同类型的日志
- space ID: 表空间的额ID
- page number: 页号
- data: 内容

## MINI Transtraction
在生成redo日志的过程中，某些修改数据的操作会产生多条redo日志，为了保证原子性，我们要么生成完整的全部redo日志 要么不生成关于这一修改的redo日志 即认为没有修改数据。

因此将redo日志以组为单位进行划分，每组的redo日志都是不可分割的，具有原子性。这里给一个不可分割的例子：
- 我们插入数据时 需要更新索引树 按照一定的顺序 在某一页中插入新数据的索引 若这页的空间不够 则需要分裂这一页的数据 并把一般的数据复制到新页上 然后再插入数据 由于分裂页了 因此需要更新上一层的目录 以及上上一层的目录.....
- 我们称待插入页面仍有空间插入新数据为乐观插入 即不需要分裂页 更新的东西少一点
- 而需要分裂页的插入为悲观插入 需要更新很多东西 这时就产生了多条的redo日志 我们将其划分为一组 

构造组：每一组的最后一条日志是一条类型为MLOF_MULTI_REC_END只有当遇到了这个类型的redo日志 才认为是一组
- 注意这个类型的日志 只有一个数据就是type 其他都没有 这个字段占8bit 同时我们之前提到过只有53中不同类型的页 因此7bit就可以表示所有的类型 因此最高位的比特被用于表示改组是否只由1条记录构成 若为1 则表示由单一的redo日志构成组 否则为多条

一个事务中包含多条语句 而每条语句又可以由多个原子操作完成(一次原子访问被称为Mini-Transtraction, 简称为mtr) 即一个条语句可以由多个mtr构成 而一个mtr表示一个组 由多条redo日志构成


## redo日志的写入
redo log block: 存储redo log的基本单位 (就类似于页是表空间中存储数据的基本单位)

结构：
- log block header:
  - LOG_BLOCK_HDR_NO: Block的编号
  - LOG_BLOCK_HDR_DATA_LEN: 表示当前block已经用了多少空间(从12开始 因为header占12字节) 若写满了 这个值为512
  - LOG_BLOCK_FIRST_REC_GROUP: 表示该block中第一个mtr生成的第一条redo日志的偏移量
  - LOG_BLOCK_CHECKPOINT_NO: 表示checkpoint的序号 用于后续写入磁盘时用到
- log block body: 
- log block trailer: LOG_BLOCK_CHECKSUM

log buffer: 由于内存和磁盘之间交互较慢，因此在buffer pool中会有一块连续的内存作为缓冲区存放日志

LOG BUFFER中就是连续的redo log block的结构

一个mtr产生的多条redo log会先暂存到一个地方，当mtr结束的时候 再将全部的redo log复制到log buffer中 保证mtr的日志的再内存中的连续性
# Chap21 redo日志 - 磁盘

checkpoint: 恢复数据时用于判断从什么地方开始恢复
flush_to_disk_lsn: 已经刷新到磁盘的redo日志的lsn
oldest_modification_lsn newest_modification_lsn: flush链表中的两个属性 前者记录该页面最早一次修改的时间 链表整体也是依据这个值进行排序 后者记录最近一次更新的lsn 就是说对已经在链表中的页面进行二次修改时 不会重复的加入页面 而是更新newest_modification_lsn这个值

# Chap22 undo日志
事务回滚 为了保持事务的原子性。有时候事务进行到一半会由于一些特殊原因中止执行，导致不符合原子性 因此需要回滚到什么都没有执行的时候，保持事务的原子性

所以我们对表中数据进行更新(指insert delete update三种操作)时需要记录一下操作的内容 方便后续回滚

事务id：当对表中的数据进行增删改时 会为事务分配id

## undo日志格式
由于undo日志 只有在增删改三种操作时才会产生 因此undo日志可以分为三种格式
- INSERT
  - 插入数据若需要回滚的话 只需要删除当初插入的数据即可 因此需要记录插入数据的主键(因为每条记录主键都是唯一的) 剩下的就是删除的操作了
  - 所以需要记录额外数据就是主键数据：<len, value>列表  因为主键可能由多列构成 所以是这样一个键值对的列表
- DELETE
  - 




# Chap24 事务的隔离级别
- 为什么要有事务隔离？<br>
因为SQL是一个服务器 可以和多个客户端相连接 所以在这个工程中可能会出现多个客户端一起对服务器中的数据进行操作的情况 即事务并发执行 <br>
而事务并发执行的过程中一方的操作可能会对另一方的操作产生影响 因此需要设置事务的隔离级别 若设置成事务之间完全没有影响 可能会降低事务执行的效率 因此我们需要允许部分影响产生 提高事务执行的效率 也就有了可以设置不同的事务隔离级别

- 事务并发执行可能遇到的情况
  - 脏写<br>
  即事务B修改了另一个未提交事务A修改过的数据 而未提交事务A随时可能会进行回滚 即使事务B修改完数据已经提交 事务A回滚后 事务B的修改也会消失 <br>
  脏写的**本质**在于事务B修改事务A修改过的值 但是此时事务A未提交 所以事务A可能会回滚 导致事务B修改的值也没了 其原因一个事务去更新了另一个还未提交的事务更新后的数据 
  - 脏读<br>
  即事务B读到了未提交事务A修改过的数据 然后A回滚了 事务B再次去读 就读不到了 即事务B读到了事务A修改过后没有提交的数据 和脏写的本质类似 不过脏读的后果不那么严重 
  - 不可重复读<br>
  即一个事务多次查询一条数据 结果每次查询到的结果都不相同 因为查询的过程中其他的事务可能一直在修改这条记录 而且修改之后的事务都提交了 就导致每次查询到的结果都不相同<br>
  不可重复读和脏读的区别在于脏读是读到了别人未提交的事务 之后就读不到这条数据了 而不可能重复读是读到了别人反复更新后的已提交的事务 
  - 幻读<br>
  幻读就是一个事务按照一些条件去查询记录 每次查询都会多出一些之前没有看见过的记录 原因在于其他事务插入了一些其他的记录 导致之后每次查询都会多一些记录<br>
  幻读的点在于每次查询的是一个范围内的数据 而且是会多出一些之前没有的记录<br> 
  若是每次查询都会少一些记录 则会被归类为不可重复读

对于隔离级别 有看到说第三种叫做可重复读 指在同一个事务内部 两次读取同一个数据是相同的 也就是说一个事务读数据的时候禁止其他事务修改这个数据？？？ 
- 其实应该是一个东西 不过对于可重复读这一隔离级别 不是通过禁止其他事务修改数据 而是一个事务开启一个readview 所以这个事务期间读到的数据不会发生变化

针对以上事务并发可能遇到的情况 设计了四种事务隔离级别
- 四种问题的严重性：脏写 > 脏读 > 不可重复读 > 幻读 ; 由于脏写这个问题太严重了 所以四种隔离级别都不允许脏写的发生
-  Read Uncommitted(读未提交)<br>
允许发生脏读 不可重复读 幻读
- Read Committed(读已提交)<br>
可能发生不可重复读 和 幻读 但不会发生脏读
- Repearable Read(可重复读)<br>
可能发生幻读 但是不可能发生脏读和不可重复读 但是通过锁也可以解决幻读的问题
- Serializable(串行化)<br>
四种问题都不会发生 实际中都不会采用这种隔离级别 严重拉低并行事务的运行效率

那么事务隔离是如何发挥作用的呢？ - MVCC机制(Multi-version concurrency control) 多版本并发控制 

简单来说会有一个版本链 然后事务会有一个readview 判断可以读取哪个版本的数据
.......待完成

# Chap25 锁
24章中讲过事务并发可能存在的问题 然后提出了事务隔离级别

事务并发对同一条记录可能会有以下几种情况
- 读-读<br>
即两个事务同时对一条记录进行查询 这并不会带来任何问题
- 写-写<br>
即两个事务同时对一条事务进行修改 这会引发脏写的问题 绝对禁止<br>
如何解决脏写问题？<br>
  - 写操作加锁 即当一个事务想要修改某条记录时 就为这条记录加锁 这样其他的时候就无法对这条记录同时进行修改 直到前一个事务完成 就相当于两个事务排队对这条记录及逆行修改 阻止了事务并发
- 读-写 / 写-读<br>
即一个事务进行读 另一个事务进行写 这可能会引发脏读 不可重复读 幻读三种情况 三者的区别这里不再赘述<br>
如何解决读-写问题？<br> 
  - 两种方法
  - 一致性读(快照读)<br>
利用MVCC进行读操作 而写操作加锁
由于利用了MVCC所以只能读到符合条件的历史版本的数据 而写操作总是对最新版本进行操作 所以二者不会冲突
  - 锁定读<br>
有些业务场景下 不允许读取历史数据(比如银行业务查询余额 只能查询到最新的余额 不能查到历史版本的余额 否则钱花了不就和没花一样) 因此需要对读操作也加锁 读和写一个时间只能进行一种 
当然这种情况下 会大大降低事务处理的效率  因此 除非业务需要 不然不会使用锁定读

## 锁
根据作用的范围可以分为行锁和表锁
根据作用分可以分为
- Shared Lock(共享锁/S锁)<br>
共享锁 即各个事务都可以拥有的锁 主要用于读记录时 
- Exclusive Lock(独占锁 排他锁 / X锁)<br>
独占锁 即只能有一个事务获取 用于修改记录时 
- Intention Shared Lock(意向共享锁 / IS锁)<br>
表级锁 用于快速的确定当前的表中是否存在行级的共享锁
- Intention Exclusive Lock(意向独占锁 / IX锁)<br>
表级锁 用于快速的确定当前表中是否存在行级的独占锁




















